{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aafc313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For licensing see accompanying LICENSE file. Copyright (C) 2024 Apple Inc. All Rights Reserved.\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b021b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "%env OPENAI_API_KEY='' #your api key\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d88ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(d, response):\n",
    "    instruction = d['instruction']\n",
    "    weight = d['component_weight'] * 1\n",
    "    d['num_of_component'] = len(d['components'])\n",
    "    for i in range(len(weight)):\n",
    "        weight[i] = str(weight[i])\n",
    "    if d['num_of_component'] == 1:\n",
    "        components = '''The first component is:' ''' + d['components'][0] + \"'\"  \n",
    "        score = '''The first component is worth ''' + weight[0] + ' scores.'\n",
    "    elif d['num_of_component'] == 2:\n",
    "        components = '''The first component is:' ''' + d['components'][0] + '''', and the second component is:' ''' + d['components'][1] + \"'\" \n",
    "        score = '''The first and second component is each worth ''' + weight[0] + ' and ' + weight[1]+ ' scores.'\n",
    "    elif d['num_of_component'] == 3:\n",
    "        components = '''The first component is:' ''' + d['components'][0] + '''', and the second component is:' ''' + d['components'][1] + '''', and the third component is:' ''' + d['components'][2] + \"'\" \n",
    "        score = '''The first second, and third component is each worth ''' + weight[0] + ', ' + weight[1]+ ' and ' + weight[2] + ' scores.'\n",
    "    elif d['num_of_component'] == 4:\n",
    "        components = '''The first component is:' ''' + d['components'][0] + '''', and the second component is:' ''' + d['components'][1] + '''', and the third component is:' ''' + d['components'][2] +  '''', and the fourth component is:' ''' + d['components'][3] + \"'\" \n",
    "        score = '''The first second, third, and fourth component is each worth ''' + weight[0] + ', ' + weight[1]+ ', ' + weight[2] + ' and ' + weight[3] + ' scores.'\n",
    "    elif d['num_of_component'] == 5:\n",
    "        components = '''The first component is:' ''' + d['components'][0] + '''', and the second component is:' ''' + d['components'][1] + '''', and the third component is:' ''' + d['components'][2] +  '''', and the fourth component is:' ''' + d['components'][3] +  '''', and the fifth component is:' ''' + d['components'][4] + \"'\" \n",
    "        score = '''The first second, third, fourth and fifth component is each worth ''' + weight[0] + ', ' + weight[1]+ ', ' + weight[2] + ', ' + weight[3] + ' and ' + weight[4] + ' scores.'      \n",
    "    return '''Here is an instruction for a multimodal LLM: ' ''' + instruction + ''' You need to grade if the response from the model follows each component of the instruction. ''' + components + ''' The response is:' '''  + response +  '''' You need to score the response and be strict. The total score ranges from 0 to 10, depending on if the response follows the instruction. ''' + score + ' List scores of each component, and the total score in one sentence in this format: score of component 1: x/2, score of component 2: y/8, total score: z/10. Then explain your reasons.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rawscore(component_type, raw_score):\n",
    "    first_sentence = raw_score.split('''.''')[0].split(''',''')\n",
    "    score_dict = {}\n",
    "    for i in range(len(first_sentence) - 1):\n",
    "        score_ = first_sentence[i].split(''':''')[1][1:].split('''/''')\n",
    "        score = int(score_[0])/int(score_[1])\n",
    "        score_dict[component_type[i]] = score\n",
    "    total_score_ = first_sentence[i+1].split(''':''')[1][1:].split('''/''')\n",
    "    total_score = int(total_score_[0])/int(total_score_[1])\n",
    "    score_dict['total_score'] = total_score\n",
    "    return score_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_dict(column_name):\n",
    "    cat_score_dict = {}\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            score_dict = process_rawscore(df['component_type'][i], df[column_name][i])\n",
    "            for key, val in score_dict.items():\n",
    "                if key not in cat_score_dict.keys():\n",
    "                    cat_score_dict[key] = [val]\n",
    "                else:\n",
    "                    cat_score_dict[key].append(val)\n",
    "        except:\n",
    "            pass\n",
    "    cat_score_dict_average = {}\n",
    "    for key, val in cat_score_dict.items():\n",
    "        cat_score_dict_average[key] = sum(val)/len(val)\n",
    "    return cat_score_dict_average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012aedb",
   "metadata": {},
   "source": [
    "***load benchmark***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('instruction_benchmark/prompt_image_file/instruction_benchmark_all.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b52f2a",
   "metadata": {},
   "source": [
    "***example evaluation on llava 1.6 13b***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_file = 'instruction_benchmark/inference_result/sft/llava_1_6_13b_mia.jsonl'\n",
    "answers = [json.loads(q) for q in open(ans_file, 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fde5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llava = pd.DataFrame(answers)\n",
    "df_llava.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886c4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llava['score_raw'] = [_ for _ in range(len(df_llava))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(df_llava))):\n",
    "    d = {}\n",
    "    for col in df.columns:\n",
    "        d[col] = df[col][i]\n",
    "    response = df_llava['text'][i]\n",
    "    image = df_llava['url'][i]\n",
    "    question =  generate_prompt(d, response)\n",
    "    generated = False\n",
    "    if df_llava['text'][i] != 'error':\n",
    "        attempt = 5\n",
    "        \n",
    "        while attempt > 0 and generated == False:\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[\n",
    "                                  {\n",
    "                                      \"role\": \"user\",\n",
    "                                      \"content\": [\n",
    "                                          {\"type\": \"text\", \"text\": question},\n",
    "                                          {\"type\": \"image_url\",\n",
    "                                            \"image_url\": df_llava['url'][i]\n",
    "                                          },\n",
    "                                      ],\n",
    "                                  }\n",
    "                              ],\n",
    "                              max_tokens=2000\n",
    "                          )\n",
    "                print(response.choices[0].message.content.strip())\n",
    "                df_llava['score_raw'][i] = response.choices[0].message.content.strip()\n",
    "                generated = True\n",
    "            except:\n",
    "                attempt -= 1\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3453b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results to the benchmark dataframe, for easier later comparison\n",
    "df['llava_1_6_13b'] = df_llava['score_raw']\n",
    "get_score_dict('llava_1_6_13b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed3d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
